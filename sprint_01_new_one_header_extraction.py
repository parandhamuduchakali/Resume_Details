# -*- coding: utf-8 -*-
"""Sprint_01_new_one - Header_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16L-8j6lWihxv5435LNuKOZBQNWqMliBh
"""

# !pip list

# !pip install -U spacy --user
# !python -m spacy download en_core_web_lg

# !pip install spacy==3.0.0 --user
# !pip install fitz
# !pip install frontend
# !pip install PyPDF2
# !pip install PyMuPDF
# !pip install newlinejson

# !pip install pdfminer.six

import spacy
import pickle
import random
import sys, fitz,re
# from spacy.language import Language
import os
from PyPDF2 import PdfFileMerger
from platform import python_version

#scientific libraries
import pandas as pd
from operator import itemgetter

# Spacy Library
from spacy.matcher import Matcher

# nltk libarary
from nltk.corpus import stopwords

import warnings
warnings.filterwarnings("ignore")

import nltk
# import re
# import spacy
from nltk.corpus import stopwords
nltk.download('stopwords')
# from spacy.tokenizer import Tokenizer
from fuzzywuzzy import fuzz

import datefinder
import numpy as np
from dateparser.search import search_dates

from datetime import datetime
from dateutil import relativedelta

import pytesseract
import pdf2image

# Check Spacy Version
!python -m spacy info

print ("===============info about python version===============")

print(python_version())

# train_data = pickle.load(open('train_data.pkl', 'rb'))

# train_data[0]

# loading Large english model
import en_core_web_lg
nlp = spacy.load("en_core_web_lg")

import glob
pdf_files = glob.glob("D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Gold_dataset/*.pdf")
pdf_files

# ## Change the name of files in directory

# path = 'D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Resumes/'
# files = os.listdir(path)

# for index, file in enumerate(pdf_files):
#     print (file)
#     os.rename(file, path +'train_' + str(index)+ '.docx')

"""### Headline Extraction"""

# import tika
# tika.initVM()
# from tika import parser
# # from tika import parser
# headers = {
# 'X-Tika-PDFextractInlineImages': 'true',
# }
# parsed = parser.from_file("28.pdf",serverEndpoint='http://localhost:9998/rmeta/text',headers=headers)
# parsed['content']

# for each_element in parsed['content'].split('\n'):
#     print(each_element)

filename = []
for fname in pdf_files:
    files = fitz.open(fname)
    filename.append(files)
# print(filename)

# !pip install tabula-py

doc = filename[41]
doc

for page in doc:
    text = page.getText('text')
    print(text)

# print(page.getText("dict"))

# # print(page.get_text("blocks"))
# blocks = page.getText("dict")["blocks"]
# blocks

#
# for word in page.get_text("words"):
#         print(word)

# html
# for line in page.get_text("html").splitlines():
#         print(line)

# # search specific word in the document
# search_term = "Work Experience"
# for pages in range(len(doc)):
#     page = doc.loadPage(pages)
#     if page.searchFor(search_term):
#         print("%s found on page %i" % (search_term, pages))

def fonts(doc, granularity=False):
    """This function will help in extracting number of counts,
    size of fonts and type of fonts from PDF documents.

    =================="---------Parameters Explanation---------"====================
    :parameter doc: PDF document to iterate through
    :type doc: <class 'fitz.fitz.Document'>
    :parameter granularity: also use 'font', 'flags' and 'color' to discriminate text
    :type granularity: bool
    :rtype: [(font_size, count), (font_size, count}], dict
    :return: most used fonts sorted by count, font style information
    =================="---------Parameters Explanation---------"====================
    """
    styles = {}
    font_counts = {}

    for page in doc:
        blocks = page.getText("dict")["blocks"]
        for b in blocks:  # iterate through the text blocks
            if b['type'] == 0:  # block contains text
                for l in b["lines"]:  # iterate through the text lines
                    for s in l["spans"]:  # iterate through the text spans
                        if granularity:
                            identifier = "{0}_{1}_{2}_{3}".format(s['size'], s['flags'], s['font'], s['color'])
                            styles[identifier] = {'size': s['size'], 'flags': s['flags'], 'font': s['font'],
                                                  'color': s['color']}
                        else:
                            identifier = "{0}".format(s['size'])
                            styles[identifier] = {'size': s['size'], 'font': s['font']}

                        font_counts[identifier] = font_counts.get(identifier, 0) + 1  # count the fonts usage

    font_counts = sorted(font_counts.items(), key=itemgetter(1), reverse=True)

    if len(font_counts) < 1:
        raise ValueError("Zero discriminating fonts found!")

    return font_counts, styles

# fonts(doc , granularity=False )

"""----------------------------------------------------------------------------------------------------------------"""

def font_tags(font_counts, styles):
    """Returns dictionary with font sizes as keys and
    tags as value.

    =================="---------Parameters Explanation---------"====================
    :parameter font_counts: (font_size, count) for all fonts occuring in document
    :type font_counts: list
    :parameter styles: all styles found in the document
    :type styles: dict
    :rtype: dict
    :return: all element tags based on font-sizes
    =================="---------Parameters Explanation---------"====================
    """

    p_style = styles[font_counts[0][0]]  # get style for most used font by count (paragraph)
    p_size = p_style['size']  # get the paragraph's size

    # sorting the font sizes high to low, so that we can append the right integer to each tag
    font_sizes = []
    for (font_size, count) in font_counts:
        font_sizes.append(float(font_size))
    font_sizes.sort(reverse=True)

    # aggregating the tags for each font size
    idx = 0
    size_tag = {}
    for size in font_sizes:
        idx += 1
        if size == p_size:
            idx = 0
            size_tag[size] = '<p>'
        if size > p_size:
            size_tag[size] = '<h{0}>'.format(idx)
        elif size < p_size:
            size_tag[size] = '<s{0}>'.format(idx)

    return size_tag

"""--------------------------------------------------------------------------------------------------------------------"""

def headers_para(doc, size_tag):
    """Scrapes headers & paragraphs from PDF
    and return texts with element tags.

    =================="---------Parameters Explanation---------"====================
    :param doc: PDF document to iterate through
    :type doc: <class 'fitz.fitz.Document'>
    :param size_tag: textual element tags for each size
    :type size_tag: dict
    :rtype: list
    :return: texts with pre-prended element tags
    =================="---------Parameters Explanation---------"====================
    """

    header_para = []  # list with headers and paragraphs
    first = True  # boolean operator for first header
    previous_s = {}  # previous span

    for page in doc:
        blocks = page.getText("dict")["blocks"]
        for b in blocks:  # iterate through the text blocks
            if b['type'] == 0:  # this block contains text

                # REMEMBER: multiple fonts and sizes are possible IN one block

                block_string = ""  # text found in block
                for l in b["lines"]:  # iterate through the text lines
                    for s in l["spans"]:  # iterate through the text spans
                        if s['text'].strip():  # removing whitespaces:
                            if first:
                                previous_s = s
                                first = False
                                block_string = size_tag[s['size']] + s['text']
                            else:
                                if s['size'] == previous_s['size']:

                                    if block_string and all((c == "|") for c in block_string):
                                        # block_string only contains pipes
                                        block_string = size_tag[s['size']] + s['text']
                                    if block_string == "":
                                        # new block has started, so append size tag
                                        block_string = size_tag[s['size']] + s['text']
                                    else:  # in the same block, so concatenate strings
                                        block_string += " " + s['text']

                                else:
                                    header_para.append(block_string)
                                    block_string = size_tag[s['size']] + s['text']

                                previous_s = s

                    # new block started, indicating with a pipe
                    block_string += " "

                header_para.append(block_string)

    return header_para

font_counts, styles = fonts(doc, granularity=False)
# print(font_counts)

# print("=================","=======================","========================","=======================","===============")

# print(styles)

# print("=================","=======================","========================","=======================","===============")

size_tag = font_tags(font_counts, styles)
# print(size_tag)

elements = headers_para(doc, size_tag)
elements
# elements = [i.rstrip(' ') for i in elements]
# print(elements)

header = ["<h1>","<h2>","<h3>","<h4>","<h5>","<h6>","<h7>","<h8>","<h9>","<h10>"]

header_index = []
for eachel_index, eachel_value in enumerate(elements):
    for i in header:
        if i in eachel_value:
            print (eachel_index,eachel_value)
            header_index.append(eachel_index)

"""# <span style="color:red">Model -1 -> Header Extraction [Not to be used for now] </span>."""

# %%time

# # Method-1 [ Using header Extraction Method]

# final_data = {}
# failed_files = []
# # nlp = en_core_web_md.load()

# pdf_files = glob.glob("D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Dataset_sprint_01/*.pdf")

# for eachFile in pdf_files:
#   try:
#     # print ("Processing *** ",eachFile)
#     doc = fitz.open(eachFile)
#     font_counts, styles = fonts(doc, granularity=False)
#     size_tag = font_tags(font_counts, styles)
#     elements = headers_para(doc, size_tag)

#     nlp_text = " ".join(elements[0:10])
#     # print(nlp_text)
#     # get basic profile detail
#     name =  get_name(nlp_text)
#     phone = get_phone(nlp_text)
#     email = get_email(nlp_text)

#     header_index = []
#     header_index_clean = []
#     value_dict = {}


#     for eachel_index, eachel_value in enumerate(elements):
#       for i in header_lookup:
#         if i in eachel_value:
#           header_index.append(eachel_index)

#     # Cleaning Index with non-required tags
#     for i in range(len(header_index)):
#       start_index = header_index[i]
#       key = elements[start_index].split(">")[1].replace('|','').strip()
#       standardized_key = lookup_key_check(key, lookup_dict)
#       if standardized_key:
#         header_index_clean.append(header_index[i])

#     # getting value for only required cleaned Indexes
#     for i in range(len(header_index_clean)):
#       # print (i)
#       # print(len(header_index_clean))
#       if i < len(header_index_clean)-1:
#         start_index = header_index_clean[i]
#         end_index = header_index_clean[i+1]
#         key = elements[start_index].split(">")[1].replace('|','').strip()
#         value = clean_value_htmltags(elements[start_index+1:end_index])

#         standardized_key = lookup_key_check(key, lookup_dict)
#         if standardized_key:
#           # print(standardized_key)
#           value_dict[standardized_key] = {}
#           value_dict[standardized_key]['Value'] = value
#       else:
#         start_index = header_index_clean[i]
#         # end_index = header_index_clean[i+1]
#         key = elements[start_index].split(">")[1].replace('|','').strip()
#         value = clean_value_htmltags(elements[start_index+1:])

#         standardized_key = lookup_key_check(key, lookup_dict)
#         if standardized_key:
#           # print(standardized_key)
#           value_dict[standardized_key] = {}
#           value_dict[standardized_key]['Value'] = value


#     # for i in range(len(header_index)-1):
#     #   start_index = header_index[i]
#     #   end_index = header_index[i+1]
#     #   key = elements[start_index].split(">")[1].replace('|','').strip()
#     #   value = clean_value_htmltags(elements[start_index+1:end_index])


#     #   # print(value)
#     #   standardized_key = lookup_key_check(key, lookup_dict)
#     #   # print(standardized_key)
#     #   if standardized_key:
#     #     value_dict[standardized_key] = {}
#     #     value_dict[standardized_key]['Value'] = value


#         #Get entities
#         # value_dict[standardized_key]['Entities'] =  extract_entities(value)

#     try:
#       if "Summary" not in value_dict.keys() and header_index:
#         value_dict['Summary'] = clean_value_htmltags(elements[header_index[0]:header_index[4]])
#     except IndexError as ie:
#         value_dict['Summary'] = ""
#         print ("Index error Occured in file -",eachFile)

#     final_data[eachFile] = value_dict
#     final_data[eachFile]['personal_details'] = {}
#     final_data[eachFile]['personal_details']['name'] = name
#     final_data[eachFile]['personal_details']['email'] = email
#     final_data[eachFile]['personal_details']['phone'] = phone
#   except Exception as e:
#     print ("Failed File ****** ",eachFile)
#     failed_files.append(eachFile)
#     print(e)

# print(len(final_data.keys()))

# df = pd.DataFrame.from_dict(final_data,orient="index").sort_index()

# TRSO = (1- df.isnull().sum(axis=1)/len(df.columns))*100
# df['TRSO'] = TRSO

# TRAO ={}

# for i in df.columns:
#     key = i
#     value = (1-df[i].isna().sum()/len(df))*100
#     TRAO[key] = value

# print("Tag V/s All observation:", TRAO)
# print("---------------------------------------------------------")
# print("Tag V/s Single Observation Average:" , TRSO.mean())

# df.head()

# na_value ={}

# # for i in df.columns:
# #     key = i
# #     value = df[i].isna().sum()/len(df)*100
# #     na_value[key] = value

# # print(na_value)

# # print("**********plot**************************")
# # pd.DataFrame.from_dict(na_value , orient = 'index').T.plot.bar()

"""## |-----------------|------------------NEW Algoritham Using TIKA codes-----------------------|--------------------|"""

lookup_dict = {
    "Work Experience": ['career path', 'organizational experience', 'professional experience', 'experience',
                        'professional summary', 'work history', 'present experience',
                        'career highlights', 'organizational scan',
                        'previous experience', 'privious experience',
                        'other experience', 'work experience',
                        'professional background','employment','internship& work experience',
                       'employment history',
                        'employment scan', "experience profile",
                        "occupational contour",'e x p e r i e n c e',
                        'confidential','career progression','experience summary',
                       'career timeline','technical experience',
                        'work  exper i ence','employment experience','pr o fessio n al expe rie n ce'],



"Education": ['education','educational qualifications', 'academic qualifications',
              'academic details', 'academia','training certificates','certification',
              'educational background','academic qualification','education and certification' ,
              'academic background','technical expertise','training certificates',
              'computer knowledge','computer literacy','e d u c a t i o n',
              'academic history','certifications','qualifications','educational documents','professional qualification',
              'courses and certifications','educat i on','education & credentials'],




"Skills": ["software skills","key skills","skills/it skills","skills","skills & abilities",
           'technical expertise','technical skills','technical proficiency',
           'professional skills' , 's k i l l s', 'proficiency skills',
           'p r o f e s s i o n a l  s k i l l s','software proficiency' ,
           'programming languages','areas of expertise','technical expertise',
           'key technologies','functional expertise','computer literacy','sk i l l s',
           'profe s s i ona l  sk i l l s','languages/libraries/ides','te ch ni c al exp e rtise',
          'too ls a n d te c hn ol og i es','knowledge base','key skills (not exhaustive)','skills & tools',
          'technical skills & interests'],




"Summary": ["summary", "objective" , "career objective" , "synopsis" ,
            "profile summary" ,"job description","highlights of qualification",
            "field of competence","a b o u t m e",'career overview','profile',
            'prof i l e s','pr o fessio n al summ ary']
#     ,

# "Date": ['january','jan' , 'february' , 'feb','march','mar','April','apr','may','may','june','jun',
#         'july','jul','august','aug','september','sep','october','oct','november','nov','december','dec']
    #,

# "projects": ['personal projects' , 'major projects', 'project experience' ,'final year project','projects worked on',
#             'projects details','projects','key projects undertaken', 'projects description','major project work',
#              'project exposure','project experience summary','data science exposure','project']

}

tags_removal_lookup = ["<p>","<s1>","<s2>","<s3>","<h3>","<h4>","<h5>","<h6>","<h7>","<h8>","<h9>","<h10>"]
header_lookup = ["<h1>","<h2>","<h3>","<h4>","<h5>","<h6>","<h7>","<h8>","<h9>","<h10>","<p>"]

header_lookup =[" "]

#"<h3>","<h4>","<h5>","<h6>","<h7>","<h8>","<h9>","<h10>"

def lookup_key_check(keytocheck, lookup_dict):
  for keyit,value in lookup_dict.items():
    for eachValue in value:
      if fuzz.token_sort_ratio(eachValue.lower(),keytocheck.lower()) > 80:
        return keyit
#       else:
#             if fuzz.ratio(eachValue.lower(),keytocheck.lower()) >80:
#                 return keyit


# def clean_value(value_list):
#   cleaned_value = []
#   for eachel in value_list:
#     if "<p>" in eachel:
#       cleaned_value.append(eachel.split("<p>")[1].strip())
#     elif "<s1>" in eachel:
#       cleaned_value.append(eachel.split("<s1>")[1].strip())
#     else:
#       cleaned_value.append(eachel.strip())
#   return cleaned_value

def clean_value_htmltags(value_list):
  cleanr = re.compile('<.*?>')
#   cleanr = re.sub('\s{2,}', " ", cleanr)
#   cleanr = cleanr.replace('➢','•').replace('❖' ,'•').replace('●','•').replace('✓','•').replace('','•').replace('*','•')
#   cleanr = cleanr.replace('◦' ,'•').replace('◆','•').replace('⚫','•')
#   cleanr = cleanr.replace('\u200b',' ').replace('\u25aa',' ').replace('\uf0b7',' ').replace('\uf0fc',' ').replace('\uf0b3',' ')
#   cleanr = cleanr.replace('\uf0c4',' ').replace('\uf02a',' ').replace('\uf028',' ').replace('\uf0d8',' ')
  cleantext = re.sub(cleanr, '', " ".join(value_list))
  return cleantext


def extract_entities(values):
  entitites = {}
  doc = nlp(values)
  for ent in doc.ents:
    if ent.label_ not in entitites:
      entitites[ent.label_] = []
      entitites[ent.label_].append(ent.text)
    else:
      entitites[ent.label_].append(ent.text)
  return entitites

# Name - Entity Extraction
nlp = spacy.load('en_core_web_lg')
matcher = Matcher(nlp.vocab , validate = True)

def get_name(resume_text):
    '''This helper function will help to Extract names
    from the text'''

    nlp_text = nlp(resume_text)
    # First name and Last name are always Proper Nouns
    pattern = [{"POS": "PROPN"}, {"POS": "PROPN"}] # Creating a Rule
    matcher.add('NAME' ,[pattern]) # Defining the Pattern
    matches = matcher(nlp_text) # find words and phrases using rules and defined pattern

    for _,start,end in matches:
        span = nlp_text[start:end]
        if 'name' not in span.text.lower():
            return span.text


# Email Entity Extraction
def get_email(resume_text):
    '''
    Helper function to extract email id from text
    '''
    email = re.findall(r"([^@|\s]+@[^@]+\.[^@|\s]+)", resume_text)
    if email:
        try:
            return email[0].split()[0].strip(';')
        except IndexError:
            return None

# Phone Number Extraction

def get_phone(resume_text , custom_regex = None):
    if not custom_regex:
        mob_num_regex = r'''(\d{3}[-\.\s]??\d{3}[-\.\s]??\d{4}|\(\d{3}\)
                        [-\.\s]*\d{3}[-\.\s]??\d{4}|\d{3}[-\.\s]??\d{4})'''
        phone = re.findall(re.compile(mob_num_regex) , resume_text)
    else:
        phone = re.findall(re.compile(custom_regex), resume_text)
    if phone:
        number = ''.join(phone[0])
        return number

# get_phone()

# Skills Extraction
def get_skills(resume_text):
    nlp_text = nlp(resume_text) #Loading the spacy english model
    noun_chunks = list(nlp_text.noun_chunks) # creating a list of base noun phrases

    #Word tokenization and removing stop words
    tokens = [token.text for token in nlp_text if not token.is_stop]

    data = pd.read_csv('skill_python.csv') # Note- addition of files required
    skills = list(data.columns.values) #Extract values

    skillset=[]

    #One-gram checking
    for token in tokens:
        if token.lower() in skills:
            skillset.append(token)

    #Bi-gram and Tri-Gram check
    for token in noun_chunks:
        token = token.text.lower().strip()
        if token in skills:
            skillset.append(token)

    return [i.capitalize() for i in set([i.lower() for i in skillset])]

# Extraction of Dates from Experience
def get_dates_from_work_exp(resume_text):
    combined_dates = []
    for eachEl in resume_text:
      match = re.search(
                  r'(?P<fmonth>\w+.\d+)\s*(\D|to)\s*(?P<smonth>\w+.\d+|present)',
                  eachEl,
                  re.I
              )
      if match:
        combined_dates.append(match.groups())
    return combined_dates

# Extraction of dates with Line
def lines_with_dates(resume_text):
    lines_with_dates = []
    lines_with_dates2 = []
    for eachEl in resume_text:
        matches = datefinder.find_dates(eachEl)
        for match in matches:
#             print(match)
            if match is not np.nan:
#                 print(match)
                lines_with_dates.append(eachEl)
                lines_with_dates=[i for n, i in enumerate(lines_with_dates) if i not in lines_with_dates[:n]]
#                 return lines_with_dates
        for line in lines_with_dates:
            dates = search_dates(line)
#             print(dates)
            if dates is not np.nan:
                lines_with_dates2.append(line)
                return lines_with_dates2


def get_number_of_months_from_dates(start_date, end_date):

    if end_date.lower() == 'present':
        end_date = datetime.now().strftime('%b %Y')
    try:
        if len(start_date.split()[0]) > 3:
            start_date = start_date.split()
            start_date = start_date[0][:3] + ' ' + start_date[1]
#             print(start_date)
        if len(end_date.split()[0]) > 3:
            end_date = end_date.split()
            end_date = end_date[0][:3] + ' ' + end_date[1]
    except IndexError:
        return 0
    try:
        start_date = datetime.strptime(str(start_date), '%b %Y')
        end_date = datetime.strptime(str(end_date), '%b %Y')
        months_of_experience = relativedelta.relativedelta(end_date, start_date)
        months_of_experience = (months_of_experience.years
                                * 12 + months_of_experience.months)
    except ValueError:
        return 0
    return months_of_experience


def get_total_experience(experience_list):

    exp_ = []
    for line in experience_list:
        experience = re.search(
            r'(?P<fmonth>\w+.\d+)\s*(\D|to)\s*(?P<smonth>\w+.\d+|present)',
            line,
            re.I
        )
        if experience:
            exp_.append(experience.groups())
    total_exp = sum(
        [get_number_of_months_from_dates(i[0], i[2]) for i in exp_]
    )
    total_experience_in_months = total_exp
    return total_experience_in_months

os.getcwd()

# df.to_csv("Sprint_01_output_04.csv")



from pdf2image import convert_from_path
import numpy as np
def pdf_read(path):
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
#     pdf_files = "D:\\resumeparser\\110-GD.pdf"
    pdf_files = path
    pages = convert_from_path(pdf_files, 500 , poppler_path = "D:/softwares/poppler-0.68.0/bin")
    for page in pages:
        page = np.array(page)
        #Converting to text
        text = pytesseract.image_to_string(page)
        return text



# pdf_pyt_output = pdf_read("D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Gold_dataset/01-GD.pdf")
# elements =' '.join(pdf_pyt_output.split())
# elements = elements.split()
# elements
# [i.strip() for i in pdf_pyt_output if i.strip()]

"""## Model-2(Using Tika)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# import tika
# tika.initVM()
# from tika import parser
# from tika import unpack
# import glob
# 
# final_data = {}
# failed_files = []
# 
# # data = pd.read_csv('skill_python.csv')
# headers = {'X-Tika-PDFextractInlineImages': 'true',}
# 
# pdf_files = glob.glob("D:/resumes/Gold_Data_Set/*GD.pdf")
# # pdf_read= pdf_read("D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Gold_dataset/*.pdf")
# 
# for eachFile in pdf_files:
#  try:
#     parsed = parser.from_file(eachFile, headers = headers)
# 
#     if parsed['content']:
#         elements = parsed['content'].split('\n')
#         elements = [i for i in elements if i]
#         elements = [i.strip() for i in elements if i.strip()]
#     else:
#         elements = pdf_read(eachFile)
#         elements =' '.join(elements.split())
#         elements = elements.split()
# #     print(elements)
#     nlp_text = " ".join(elements[0:100])
#     nlp_text_skills = " ".join(elements)
#     name =  get_name(nlp_text)
#     phone = get_phone(nlp_text)
#     email = get_email(nlp_text)
#     core_skills = get_skills(nlp_text_skills)
#     combined_dates = get_dates_from_work_exp(elements)
#     line_with_dates = lines_with_dates(elements)
# 
# 
#     header_index = []
#     header_index_clean = []
#     value_dict = {}
# 
# 
#     header_index_clean = []
#     for eachSignleElement_index, eachSignleElement_value  in enumerate(elements):
# #         for i in header_lookup:
# #             if i in eachSignleElement_value:
#         key = eachSignleElement_value.strip()
#         standardized_key = lookup_key_check(key, lookup_dict)
# #
#         if standardized_key:
#             header_index_clean.append(eachSignleElement_index)
# 
#     for i in range(len(header_index_clean)):
#         if i < len(header_index_clean)-1:
#             start_index = header_index_clean[i]
#             end_index = header_index_clean[i+1]
#             key = elements[start_index]
# 
#             value = elements[start_index+1:end_index]
#             standardized_key = lookup_key_check(key, lookup_dict)
# 
#             if standardized_key:
#                 value_dict[standardized_key] = {}
#                 value_dict[standardized_key]['Value'] = value
#         else:
#             start_index = header_index_clean[i]
#             key = elements[start_index]
#             value = elements[start_index+1:]
#             standardized_key = lookup_key_check(key, lookup_dict)
#             if standardized_key:
#                 value_dict[standardized_key] = {}
#                 value_dict[standardized_key] = value
#     # Summary Extraction
#     try:
#       if "Summary" not in elements:
#         value_dict["Summary"] = clean_value_htmltags(elements[6:10])
#     except IndexError as ie:
#         value_dict["Summary"] = ""
#         print("index error occured in file -" , eachFile)
# 
#         #Get entities
# #     value_dict[standardized_key]['Entities'] =  extract_entities(''.join(value))
# 
#     # Core_skills Extraction
#     final_data[eachFile] = value_dict
#     final_data[eachFile]['core_skills'] ={}
#     final_data[eachFile]['core_skills'] = core_skills
# 
#     #Date Extraction
# #     final_data[eachFile] = value_dict
#     final_data[eachFile]['combined_dates'] ={}
#     final_data[eachFile]['combined_dates']['combined_dates'] = combined_dates
# 
#     #Line with Date Extraction
# #     final_data[eachFile] = value_dict
#     final_data[eachFile]['line_with_dates'] ={}
#     final_data[eachFile]['line_with_dates']['line_with_dates'] = line_with_dates
# 
#     # Personal details Extraction
#     final_data[eachFile] = value_dict
#     final_data[eachFile]['personal_details'] = {}
#     final_data[eachFile]['personal_details']['name'] = name
#     final_data[eachFile]['personal_details']['email'] = email
#     final_data[eachFile]['personal_details']['phone'] = phone
# 
#  except Exception as e:
#     print ("Failed File ****** ",eachFile)
#     failed_files.append(eachFile)
#     print(e)
# 
# 
# df1 = pd.DataFrame.from_dict(final_data,orient="index").sort_index(ascending = True)
# 
# 
# TRSO = (1- df1.isnull().sum(axis=1)/len(df1.columns))*100
# df1['TRSO'] = TRSO
# 
# TRAO ={}
# 
# for i in df1.columns:
#     key = i
#     value = (1-df1[i].isna().sum()/len(df1))*100
#     TRAO[key] = value
# 
# print("Tag V/s All observation:", TRAO)
# print("---------------------------------------------------------")
# print("Tag V/s Single Observation Average:" , TRSO.mean())
# 
# df1

# df1.head(5)

# Extraction of dates with Line
def lines_with_dates_s(resume_text):
    lines_with_dates = []
    lines_with_dates2 = []
    for eachEl in resume_text:
        matches = datefinder.find_dates(eachEl)
        for match in matches:
#             print(match)
            if match is not np.nan:
#                 print(match)
                lines_with_dates.append(eachEl)
                lines_with_dates=[i for n, i in enumerate(lines_with_dates) if i not in lines_with_dates[:n]]
                return lines_with_dates
        for line in lines_with_dates:
            dates = search_dates(line)
#             print(dates)
            if dates is not np.nan:
                lines_with_dates2.append(line)
                return lines_with_dates2



from dateutil.parser import parse
from datetime import date
from datetime import datetime


# from_date = parse('16-12-2016')

# end_date = parse('16-12-2019')

number_of_days = end_date-from_date


def get_diff_month(d1, d2):
    from dateutil.parser import parse
    d1 = parse(d1)
    d2 = parse(d2)
    if d2 == "till":
        d2 = datetime.datetime.now()
    year = (d1.year - d2.year) * 12 + d1.month - d2.month
    year = year//12
    return year



print(get_diff_month("16-12-2019","16-12-2016"))


def get_dates_from_work_exp(resume_text):
    combined_dates = []
    for eachEl in resume_text:
      match = re.search(
                  r'(?P<from>\w+.\d+)\s*(to)\s*(?P<smonth>\w+.\d+.\d+|present|Till Date|till date)',
                  eachEl,
                  re.I
              )
      if match:
        combined_dates.append(match.groups())
    return combined_dates

#organisation with dates extraction
exp_data = []
for i in list(df1['Work Experience']):
    if type(i)==float:
        exp_data.append({'Value':'Nan'})
    else:
        exp_data.append(i)
exp_data = exp_data[4]["Value"]
date = lines_with_dates(exp_data)
date = get_dates_from_work_exp(date)
try:
    from_date = [value for value in date[0]][0]
    end_date = [value for value in date[0]][2]
    months = get_diff_month(end_date,from_date)
    print(months)
except Exception as e:
    print(f'{e}')
############################################################

















#organisation with dates extraction
# exp_data = []
# for i in list(df1['Work Experience']):
#     if type(i)==float:
#         exp_data.append({'Value':'Nan'})
#     else:
#         exp_data.append(i)
# print(exp_data[0])
# df1['org_with_dates'] = list(map(lambda x:lines_with_dates(x['Value']),exp_data))


# duration = []
# for i in list(df1['Work Experience']):
#     if type(i)==float:
#         duration.append({'Value':'Nan'})
#     else:
#         duration.append(i)

# df1['work_ex_duration'] = list(map(lambda x:get_dates_from_work_exp(x['Value']),duration))


# total_exp =[]
# for i in list(df1['Work Experience']):
#     if type(i)==float:
#         total_exp.append({'Value':'Nan'})
#     else:
#         total_exp.append(i)

# df1['total_exp_in_months'] = list(map(lambda x:get_total_experience(x['Value']),total_exp))


# # # Education with dates extraction
# edu_data =[]
# for i in list(df1['Education']):
#     if type(i)==float:
#         edu_data.append({'Value':'Nan'})
#     else:
#         edu_data.append(i)

# df1['edu_with_dates'] = list(map(lambda x:lines_with_dates(x['Value']),edu_data))





list(df1['edu_with_dates'])

"""##### |-------------|--JSON Conversion Section--|------------|"""

import newlinejson as nlj
import json
file = open('WorkExp_sw_02.jsonl','w')
writer = nlj.Writer(file)

for i in range(len(df1)):
    if type(df1['Work Experience'][i]) == str:
        writer.write(dict(df1.index[i],df1['Work Experience'][i]['Value']).strip())
#         print(dict(df1.index[i],df1['Work Experience'][i]['Value']).strip())
    else:
        writer.write({df1.index[i]:df1['Work Experience'][i]})
#         print({df1.index[i]:df1['Work Experience'][i]})
file.close()



os.getcwd()

df1.to_csv("Resume_228_annot.csv")

# df1['Work Experience'].to_json('df1_work_exp_output_01', orient = 'index')

# import json
# json_data = json.dumps(final_data)

# json_data

# with open('data.txt', 'w') as outfile:
#     json.dump(json_data, outfile)

# df1.to_csv("Tika_Algoritham_output_02.csv")

# os.getcwd()

# pd.DataFrame.from_dict(final_data,orient="index").to_csv("output_06.csv")

# Manual Extraction of resume
resume_data = pd.read_excel ("Book2.xlsx")
#setting unnamed:0 as index
resume_data = resume_data.set_index('Unnamed: 0')

#to compare strings concatenated two dataframes
string_matching_data = pd.concat([df1, resume_data], axis=1).reindex(df1.index)
string_matching_data.head()

# String matching[TPSO- TAG Precision for Tags v/s resume_Tags]
string_matching_data['TPSO_work_ex'] = string_matching_data.apply(lambda x: fuzz.token_sort_ratio(x['Work Experience']
                                                                                                , x['Work Experience_01']), axis = 1)


string_matching_data['TPSO_Education'] = string_matching_data.apply(lambda x: fuzz.token_sort_ratio(x['Education']
                                                                                                , x['Education_01']), axis = 1)



string_matching_data['TPSO_skills'] = string_matching_data.apply(lambda x: fuzz.token_sort_ratio(x['Skills']
                                                                                                , x['skills_01']), axis = 1)


string_matching_data['TPSO_personal_details'] = string_matching_data.apply(lambda x: fuzz.token_sort_ratio(x['personal_details']
                                                                                                , x['personal_details_01']), axis = 1)


string_matching_data.head()

"""## Debug Part- Tika Model"""

eachFile ="my_file"
import tika
tika.initVM()
from tika import parser
# from tika import parser
headers = {'X-Tika-PDFextractInlineImages': 'true',}
# pdf_files = glob.glob("D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Dataset_sprint_01/*.pdf")

parsed = parser.from_file('D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Gold_dataset\\01_DS.pdf',
                          headers = headers)

elements = parsed['content'].split('\n')
elements = [i for i in elements if i]
elements = [i.strip() for i in elements if i.strip()]


nlp_text = " ".join(elements[0:100])
nlp_text_skills = " ".join(elements)
# print(nlp_text)
# get basic profile detail
nlp_text = " ".join(elements[0:100])
nlp_text_skills = " ".join(elements)
name =  get_name(nlp_text)
phone = get_phone(nlp_text)
email = get_email(nlp_text)
core_skills = get_skills(nlp_text_skills)
combined_dates = get_dates_from_work_exp(elements)
line_with_dates = lines_with_dates(elements)

final_data={}
header_index = []
value_dict = {}


header_index_clean = []
for eachSignleElement_index, eachSignleElement_value  in enumerate(elements):
#     for i in header_lookup:
#         if i in eachSignleElement_value:
    key = eachSignleElement_value
    standardized_key = lookup_key_check(key, lookup_dict)
    if standardized_key:
        header_index_clean.append(eachSignleElement_index)
        print(standardized_key)
print(header_index_clean)

# for i in range(len(header_index_clean)-1):
#   start_index = header_index_clean[i]
#   end_index = header_index_clean[i+1]
# #   key = elements[start_index].split(">")[1].replace('|','').strip()
#   key = elements[start_index]
# #   print(key)
# #   print("*************************************************")
#   value = elements[start_index+1:end_index]


#   # print(value)
#   standardized_key = lookup_key_check(key, lookup_dict)

# #   print(standardized_key)
#   if standardized_key:
#     value_dict[standardized_key] = {}
#     value_dict[standardized_key]['Value'] = value

for i in range(len(header_index_clean)):
    if i < len(header_index_clean)-1:
        start_index = header_index_clean[i]
        end_index = header_index_clean[i+1]
        key = elements[start_index]
        value = elements[start_index+1:end_index]
        standardized_key = lookup_key_check(key, lookup_dict)

        if standardized_key:
            value_dict[standardized_key] = {}
            value_dict[standardized_key]['Value'] = value
        print(standardized_key)
    else:
        start_index = header_index_clean[i]
        key = elements[start_index]
        value = elements[start_index+1:]
        standardized_key = lookup_key_check(key, lookup_dict)

        if standardized_key:
            value_dict[standardized_key] = {}
            value_dict[standardized_key]['Value'] = value
        print(standardized_key)
# try:
#   if "Summary" not in value_dict.keys() and header_index:
#     value_dict['Summary'] = clean_value_htmltags(elements[header_index[0]:header_index[1]])
# except IndexError as ie:
#       value_dict['Summary'] = ""
#       print ("Index error Occured in file -")

# Extraction of Dates

try:
  if "Summary" not in elements:
    value_dict["Summary"] = clean_value_htmltags(elements[0:10])
except IndexError as ie:
    value_dict["Summary"] = ""
    print("index error occured in file -" , eachFile)


# Core_skills Extraction
final_data[eachFile] = value_dict
final_data[eachFile]['core_skills'] ={}
final_data[eachFile]['core_skills']['core_skills'] = core_skills

#Date Extraction
#     final_data[eachFile] = value_dict
final_data[eachFile]['combined_dates'] ={}
final_data[eachFile]['combined_dates']['combined_dates'] = combined_dates

#Line with Date Extraction
#     final_data[eachFile] = value_dict
final_data[eachFile]['line_with_dates'] ={}
final_data[eachFile]['line_with_dates']['line_with_dates'] = line_with_dates

final_data[eachFile] = value_dict
final_data[eachFile]['personal_details'] = {}
final_data[eachFile]['personal_details']['name'] = name
final_data[eachFile]['personal_details']['email'] = email
final_data[eachFile]['personal_details']['phone'] = phone

final_data

elements = " ".join(elements)
elements

"""## Resume_Matcher- Experiment-1"""

def get_skills(resume_text):
    nlp_text = nlp(resume_text)
    noun_chunks = list(nlp_text.noun_chunks)

    #Word tokenization and removing stop words
    tokens = [token.text for token in nlp_text if not token.is_stop]

    data = pd.read_csv('skill_python.csv')
    skills = list(data.columns.values)

    skillset=[]

    #One-gram checking
    for token in tokens:
        if token.lower() in skills:
            skillset.append(token)

    #Bi-gram and Tri-Gram check
    for token in noun_chunks:
        token = token.text.lower().strip()
        if token in skills:
            skillset.append(token)

    return [i.capitalize() for i in set([i.lower() for i in skillset])]

skill = get_skills(elements)
print(skill)

job_description = '''

Coding knowledge and experience with several languages: C, C++, Java,
JavaScript, etc.
Knowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.
Experience querying databases and using statistical computer languages: R, Python, SLQ, etc.
Experience using web services: Redshift, S3, Spark, DigitalOcean, etc.
Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.
Experience analyzing data from 3rd party providers: Google Analytics, Site Catalyst, Coremetrics, Adwords, Crimson Hexagon, Facebook Insights, etc.
Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, Gurobi, MySQL, etc.
Experience visualizing/presenting data for stakeholders using: Periscope, Business Objects, D3, ggplot, etc.

'''

required_skillset = get_skills(job_description)
print(required_skillset)

from sklearn.metrics.pairwise import cosine_similarity
resume_skillset = []
for item in skill:
    resume_skillset.append(str.lower(item))

jd_skillset=[]
for item in required_skillset:
    jd_skillset.append(str.lower(item))
# print(jd_skillset)


# print("***********************|JOB DESCRIPTION|*********************")
# print("Job_Description:",job_description)



# print("*************************|Required Skill_set|***********************************")
print("||Required Skill_set||*********** " ,jd_skillset)
# print("************************|resume_skillset|****************************************")
print("||resume_skillset||",resume_skillset)

# print("*************************||match_skills||***********************************")
match_skills = set.intersection(set(resume_skillset), set(jd_skillset))
print("||match_skills||**********" ,match_skills)

# print("*************************|PERCENTAGE OF MATCHING SKILLS with JD|***********************************")

Res_skillset = ", ".join( repr(e) for e in resume_skillset )
JD_skillset = ", ".join( repr(e) for e in jd_skillset)
text = [Res_skillset,JD_skillset]
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
count_matrix = cv.fit_transform(text)
matchPercentage = cosine_similarity(count_matrix)[0][1] * 100
matchPercentage = round(matchPercentage, 2)
print("Resume matches about "+ str(matchPercentage)+ "% of the job description.")

"""## Job_Matcher- Experiment-2"""

# !pip install jsonlines
# !pip install textract

import en_core_web_lg
import pandas as pd
import os
# import nl_core_news_sm
from spacy.pipeline import EntityRuler
from spacy import displacy
import seaborn as sns
import matplotlib.pyplot as plt
import jsonlines
import textract
# import PyPDF2
# import en_core_web_sm

import tika
tika.initVM()
from tika import parser
from tika import unpack

final_data = {}
failed_files = []

# data = pd.read_csv('skill_python.csv')
headers = {'X-Tika-PDFextractInlineImages': 'true',}
files = glob.glob("D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Gold_dataset/*.pdf")

def get_elements(file):
  try:
    parsed = parser.from_file(file, headers = headers)
    elements = parsed['content'].split('\n')
    elements = [i for i in elements if i]
    elements = [i.strip() for i in elements if i.strip()]
    return elements
  except Exception as e:
    return {"*File not in proper format*"}

# get_elements(files[0])

PROJECT_DIR = "D:/Kreaas Project/Resume Screening Project_01/"


skill_pattern_path = PROJECT_DIR + "Resume_Matcher/skill_patterns.jsonl"

with jsonlines.open(skill_pattern_path) as f:
    created_entities = [line['label'].upper() for line in f.iter()]

# File Extension. set as 'pdf' or as 'doc(x)'
extension = 'pdf'


def extract_text_from_pdf(file):
    '''Opens and reads in a PDF file from path'''
    elements = get_elements(file)

    return str(elements).replace("\\n", "")

def extract_text_from_word(filepath):
    '''Opens en reads in a .doc or .docx file from path'''

    txt = textract.process(filepath).decode('utf-8')

    return txt.replace('\n', ' ').replace('\t', ' ')

nlp.add_pipe("entity_ruler")
nlp.get_pipe("entity_ruler").from_disk(skill_pattern_path)

def visualize_entity_ruler(entity_list, doc):

    options = {"ents": entity_list}
    displacy.render(doc, style='ent', options=options)

def create_skill_set(doc):

    return set([ent.label_.upper()[6:] for ent in doc.ents if 'skill' in ent.label_.lower()])

def create_skillset_dict(resume_names, resume_texts):
    skillsets = [create_skill_set(resume_text) for resume_text in resume_texts]
    return dict(zip(resume_names, skillsets))

def match_skills(vacature_set, cv_set, resume_name):

    if len(vacature_set) < 1:
        print('could not extract skills from job offer text')
    else:
        pct_match = round(len(vacature_set.intersection(cv_set[resume_name])) / len(vacature_set) * 100, 0)
        print(resume_name + " has a {}% skill match on this job offer".format(pct_match))
        print('Required skills: {} '.format(vacature_set))
        print('Matched skills: {} \n'.format(vacature_set.intersection(skillset_dict[resume_name])))

        return (resume_name, pct_match)

def create_tokenized_texts_list(extension):
    resume_texts, resume_names = [], []
    for resume in list(filter(lambda x: extension in x, os.listdir(PROJECT_DIR + "Resume_Matcher/CV"))):
        if extension == 'pdf':
            resume_texts.append(nlp(extract_text_from_pdf(PROJECT_DIR+ "Resume_Matcher/CV/" + resume)))
        elif 'doc' in extension:
            resume_texts.append(nlp(extract_text_from_word(PROJECT_DIR + "Resume_Matcher/CV/" + resume)))

        resume_names.append(resume.split('_')[0].capitalize())

    return resume_texts, resume_names

resume_texts, resume_names = create_tokenized_texts_list(extension)

skillset_dict = create_skillset_dict(resume_names, resume_texts)

job_description = """



Coding knowledge and experience with several languages: C, C++, Java,
		JavaScript, etc.
		Knowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.
		Experience querying databases and using statistical computer languages: R, Python, SLQ, etc.
		Experience using web services: Redshift, S3, Spark, DigitalOcean, etc.
		Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.
		Experience analyzing data from 3rd party providers: Google Analytics, Site Catalyst, Coremetrics, Adwords, Crimson Hexagon, Facebook Insights, etc.
		Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, Gurobi, MySQL, etc.
		Experience visualizing/presenting data for stakeholders using: Periscope, Business Objects, D3, ggplot, etc.


"""

vacature_skillset = create_skill_set(nlp(job_description))

# Create a list with tuple pairs containing the names of the candidates and their match percentage
match_pairs = [match_skills(vacature_skillset, skillset_dict, name) for name in skillset_dict.keys()]











cv.vocabulary_.keys()



"""## String Matching Part"""

# Manual Extraction of resume
resume_data = pd.read_excel ("Book1.xlsx")
#setting unnamed:0 as index
resume_data = resume_data.set_index('Unnamed: 0')

#to compare strings concatenated two dataframes
string_matching_data = pd.concat([df, resume_data], axis=1).reindex(df.index)
string_matching_data.head()

# String matching[TPSO- TAG Precision for Tags v/s resume_Tags]
string_matching_data['TPSO_work_ex'] = string_matching_data.apply(lambda x: fuzz.token_sort_ratio(x['Work Experience']
                                                                                                , x['Work Experience_01']), axis = 1)


string_matching_data['TPSO_Education'] = string_matching_data.apply(lambda x: fuzz.token_sort_ratio(x['Education']
                                                                                                , x['Education_01']), axis = 1)



string_matching_data['TPSO_skills'] = string_matching_data.apply(lambda x: fuzz.token_sort_ratio(x['Skills']
                                                                                                , x['Skills_02']), axis = 1)


string_matching_data['TPSO_personal_details'] = string_matching_data.apply(lambda x: fuzz.token_sort_ratio(x['personal_details']
                                                                                                , x['personal_details_01']), axis = 1)


string_matching_data.head()

sum(string_matching_data['TPSO_work_ex']<20)

string_matching_data.head()

# string_matching_data.dtypes

bins = [0, 10, 20, 30, 40, 50,60,70,80,90, 100]
string_matching_data['binned_work_ex'] = pd.cut(string_matching_data['TPSO_work_ex'], bins)
string_matching_data['binned_education'] = pd.cut(string_matching_data['TPSO_Education'], bins)
string_matching_data['binned_skills'] = pd.cut(string_matching_data['TPSO_skills'], bins)
string_matching_data['binned_personal_details'] = pd.cut(string_matching_data['TPSO_personal_details'], bins)
string_matching_data.head()

string_matching_data['binned_work_ex'].value_counts(sort=False,ascending=True).plot(kind ='bar')

string_matching_data['binned_education'].value_counts(sort=False,ascending=True).plot(kind = 'bar')

string_matching_data['binned_skills'].value_counts(sort=False,ascending=True).plot(kind = 'bar')

string_matching_data['binned_personal_details'].value_counts(sort=False,ascending=True).plot(kind = 'bar')

import pywedge as pw
mc = pw.Pywedge_Charts(string_matching_data, c=None, y = 'work_ex_ratio')
charts = mc.make_charts()

#string_matching_data['work_ex_ratio'].mean()
# string_matching_data['Education_ratio'].mean()
# string_matching_data['skills_ratio'].mean()
# string_matching_data['personal_details_ratio'].mean()
# fuzz.token_sort_ratio(gold_data.iloc[2]['Work Experience'],' '.join(df.iloc[2]['Work Experience']['Value']))


# Tag specific Tag Precision
tag_precision = [{'TPSO_work_ex': string_matching_data['TPSO_work_ex'].mean(),
                 'TPSO_Education': string_matching_data['TPSO_Education'].mean(),
                     'TPSO_skills':string_matching_data['TPSO_skills'].mean(),
                        'TPSO_personal_details':string_matching_data['TPSO_personal_details'].mean()}]

df_tp = pd.DataFrame(tag_precision)
df_tp.plot(kind = 'bar')







"""# Debug Part"""

doc = fitz.open("D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Dataset_sprint_01\\31.pdf")
font_counts, styles = fonts(doc, granularity=False)
size_tag = font_tags(font_counts, styles)
elements = headers_para(doc, size_tag)

elements

# def lookup_key_check(keytocheck, lookup_dict):
#   for keyit,value in lookup_dict.items():
#     for eachValue in value:
# #         if keyit == 'Skills':
# #             print(eachValue.lower())
# #             print(keytocheck.lower())
# #             print("***********")

#           if fuzz.token_sort_ratio(eachValue.lower(),keytocheck.lower()) > 80:
#             return keyit
# #       else:
# #             if fuzz.partial_ratio(eachValue.lower(),keytocheck.lower()) >80:
# #                 return keyit
# # print(keyit)

# lookup_key_check('technical skills',lookup_dict )



final_data

elements

# final_data

elements = [i for i in elements if i]

elements[0:10]

final_data

elements

!pip install PyPDF2

!pip show PyPDF2

from PyPDF2 import PdfFileReader

from pathlib import Path
pdf_path = ("D:/Kreaas Project/Resume Screening Project_01/Talentsia_Project/Sprint_01/Dataset_sprint_01\\28.pdf")

pdf = PdfFileReader(str(pdf_path))
# print(pdf)
# print(pdf.getNumPages())
print(pdf.documentInfo)
print("************************************")
print(pdf.documentInfo.title)

first_page = pdf.getPage(0)
# first_page
# type(first_page)
first_page.extractText()

for page in pdf.pages:
    print(page.extractText())













# import yaml

# stream = open("work_exp_Lookups.yaml", 'r')
# dictionary = yaml.load(stream)
# for key, value in dictionary.items():
#     print (key + " : " + str(value))

heads = [elements[i].split('>')[1].lower() for i in header_index]
# print(heads)

cnt=0
for i in range(1,len(header_index)):
    start_index = header_index[i]
    item = elements[start_index].split('>')[1].lower().rstrip(' ')
#     print(item)

    try:
        print(section_lookup[section_lookup.index(item)])
        if item in section_lookup :

            cnt+=1

            print(heads.index(item),156)
            elements[header_index[heads.index(item)]] = '<SH{0}>{1}'.format(cnt,item.rstrip(' '))

    except ValueError as e:
        continue


# print(elements)

for i in elements:
    print(i)

section_header = ["<SH1>","<SH2>","<SH3>","<SH4>"]

section_header_index = []
for section_eachel_index, section_eachel_value in enumerate(elements):
    for i in section_header:
        if i in section_eachel_value:
            print (section_eachel_index,section_eachel_value)
            section_header_index.append(section_eachel_index)

type(section_header_index)

section_value_dict = {}

for i in range(0,len(section_header_index)-1):
    print(i)
    start_index = section_header_index[i]
    end_index = section_header_index[i+1]
    print(end_index)
    print(start_index)

    key = elements[start_index].split('>')[1].strip()
    value = elements[start_index+1:end_index]
    print("**[KEY]**:",key)
    print("[VALUE]",value)

    new_val = []
    for EachEl in value:
        if '<h' in EachEl:
            new_val.append(EachEl.replace('<h','').strip())
        else:
            new_val.append(EachEl.strip())
    while('' in new_val) :
        new_val.remove('')
    section_value_dict[re.sub(r'[^\w\s]','',key).strip()] = " ".join(new_val)

section_value_dict.keys()

section_value_dict.values()

pd.DataFrame.from_dict(section_value_dict,orient="index").T

















import re
import json

section_value_dict ={}
for i in range(len(section_header_index)-1):
    start_index = section_header_index[i]
    end_index = section_header_index[i+1]
#     print("********",elements[start_index])
#     print(elements[start_index+1:end_index])
    key = elements[start_index].split(">")[1]#.replace('|','')
    value = elements[start_index+1:end_index]
    print(key)
    print("****************")
    print(value)
#     header_val = []
#     for each_h in value:
#         if '<h' in each_h:
#             header_val.append(each_h.replace('<h{},').strip())
    new_val = []
    for EachEl in value:
        if '<h' in EachEl:
            new_val.append(EachEl.replace('<h','').strip())
        else:
            new_val.append(EachEl.strip())
    while('' in new_val) :
        new_val.remove('')
    section_value_dict[re.sub(r'[^\w\s]','',key).strip()] = " ".join(new_val)











import re
import json
value_dict = {}
for i in range(len(header_index)-1):
    start_index = header_index[i]
    end_index = header_index[i+1]
    print("********", elements[start_index])
#     print(elements[start_index+1:end_index])
    key = elements[start_index].split(">")[1].replace('|','')
    value = elements[start_index+1:end_index]
#     print(key)
#     print("************")
#     print(value)
    new_val = []
    for EachEl in value:
        if '<p>' in EachEl:
            new_val.append(EachEl.replace('<p>','').strip())
        else:
            new_val.append(EachEl.strip())
    while('' in new_val) :
        new_val.remove('')
    value_dict[re.sub(r'[^\w\s]','',key).strip()] = " ".join(new_val)

value_dict.items()

value_dict.keys()



value_dict['education']

# import json

# DS_01_json = json.dumps(value_dict)
# print(DS_01_json)

with open('09_DS.txt', 'w') as json_file:
    json.dump(value_dict, json_file)

value_dict.values()

from spacy.matcher import Matcher



nlp = spacy.load("en_core_web_lg")

matcher = Matcher(nlp.vocab)
pattern = [[{"LOWER": "Work_experience"}]]
matcher.add("organizational experience",pattern)


def replace_word(orig_text, replacement):
    tok = nlp(orig_text)
    text = ''
    buffer_start = 0
    for _, match_start, _ in matcher(tok):
        if match_start > buffer_start:  # If we've skipped over some tokens, let's add those in (with trailing whitespace if available)
            text += tok[buffer_start: match_start].text + tok[match_start - 1].whitespace_
        text += replacement + tok[match_start].whitespace_  # Replace token, with trailing whitespace if available
        buffer_start = match_start + 1
    text += tok[buffer_start:].text
    return text

replace_word("experience", "Work_experience")

value_dict.keys()

value_dict.items()

# The below function replaces any number of matches (found with spaCy), keeps the same whitespacing as the original text, and appropriately handles edge cases (like when the match is at the beginning of the text):

import spacy
from spacy.matcher import Matcher

nlp = spacy.load("en_core_web_lg")

matcher = Matcher(nlp.vocab)
matcher.add("organizational experience",[{"LOWER": "experience"}])

def replace_word(orig_text, replacement):
    tok = nlp(orig_text)
    text = ''
    buffer_start = 0
    for _, match_start, _ in matcher(tok):
        if match_start > buffer_start:  # If we've skipped over some tokens, let's add those in (with trailing whitespace if available)
            text += tok[buffer_start: match_start].text + tok[match_start - 1].whitespace_
        text += replacement + tok[match_start].whitespace_  # Replace token, with trailing whitespace if available
        buffer_start = match_start + 1
    text += tok[buffer_start:].text
    return text

replace_word("experience", "Work_experience")


# >>> replace_word("Hi this dog is my dog.", "Simba")
# Hi this Simba is my Simba.



for i in val_dict:
    i.startswith("<h") in val_dict
    print(i)

from html.parser import HTMLParser
tags = []
data_ = []
class MyHTMLParser(HTMLParser):
    global tags
    def handle_starttag(self, tag, attrs):
#         print("Encountered a start tag:", tag)
        if tag.isalnum():
            tags.append('tag__{0}'.format(tag))

#     def handle_endtag(self, tag):
#         print("Encountered an end tag :", tag)

    def handle_data(self, data):
        print("Encountered some data  :", data)
        if data.isalnum():
            data_.append('data__{0}'.format(data))

#     return tags

parser = MyHTMLParser()
tag_data = parser.feed(' '.join(elements).replace('|',''))

# data_tags = []
# for item in data_:
#     item = item.split('data__')[1]
# #     print(item)
#     if item.isalnum():
#         data_tags.append(item)

# header_data = []
# for item in tags:
#     item = item.split('tag__')[1]
# #     print(item)
#     if item.isalnum():
#         header_data.append(item)
# data_dict = {}s
# for i in tags:
#     i = i.split('__')
print(len(tags),len(data))
#     if i[0] =='tag':
#         data[i[1]] = []

parser

soup = BeautifulSoup(text, 'html')

for heading in soup.find_all(["h1"]):
    if heading.text.strip():
        header_names=print(heading.text.strip())

from bs4 import BeautifulSoup
from lxml import html

bs = BeautifulSoup(text, 'html.parser')
print(bs.find_all(["h1", "h2", "h3", "h4", "h5", "h6"]))













"""# <span style="color:red">Below line of codes Not to be used for now </span>.

### <span style="color:green"> **Pdf to text conversion -- `Pipeline-1`**</span>
"""

def PDFtoTextconverter(fname):

    '''This function is for PDF to text conversion with all designer bullet points
    to simple bullet points along with local white spaces conversion to simple spaces'''

    doc = fitz.open(fname)
    text = ""
    for page in doc:
        review = re.sub('^rt|http.+?"', ' ', text) #(@[A-Za-z0-9]+)| ^0-9A-Za-z
        text = review + str(page.getText())
        tx = " ".join(text.split())

        '''Conversion of Designer bullet points to simple bullet points'''
        tx = tx.replace('➢','•').replace('❖' ,'•').replace('●','•').replace('✓','•').replace('','•').replace('*','•')
        tx = tx.replace('◦' ,'•').replace('◆','•').replace('⚫','•')

        ##---------Can be used in future (if required) ---------------#
#         tx = tx.replace('●','•')
        ##--------Can be used in future (if required) ----------------#


        '''conversion of private white spaces to simple spaces'''
        tx = tx.replace('\u200b',' ').replace('\u25aa',' ').replace('\uf0b7',' ').replace('\uf0fc',' ').replace('\uf0b3',' ')
        tx = tx.replace('\uf0c4',' ').replace('\uf02a',' ').replace('\uf028',' ').replace('\uf0d8',' ')
    return tx


# Defining a loop to convert multiple files to text format:
data = []
for root, dirs,files in os.walk('Data/HRG_Resume/'):
    for filename in files:
        if filename.endswith('pdf'):
            path = os.path.join(root,filename)
            text = PDFtoTextconverter(path).lower()

            ##----------------Can be used in future (if required) ------------------------#
#             unnecessary_characters = ' '.join(list(set(re.sub('[A-Za-z0-9@_+-.]','',text).split())))
#             text = ' '.join(list(map(lambda words : ''.join(list(filter(lambda a :(a not in unnecessary_characters), list(words)))),text.split()))).replace('  ',' ')
            ##--------------Can be used in future (if required) -------------------------#

            file = open('Data/HRG_Resume/{0}.txt'.format(filename.split('.')[0]), 'w')
            file.write(text)
            file.close()

"""### <span style="color:green"> **Text to sentences - `Pipleine-2`**</span>."""

'''This function will help in creating text files to sentences'''

for root, dirs,files in os.walk('Data/HRG_Resume/txt_format'):
    sentences=[]
    for filename in files[:11]:
#         print(filename,)
        if filename.endswith('txt'):
            print(filename)
            path = os.path.join(root,filename)
            text_file= open(path,'r')
            doc = nlp(text_file.read())
            sentences.append([sent.text.strip() for sent in doc.sents])

# print(sentences)

"""### <span style="color:green"> **senetences to jsonl conversion -- `Pipeline-3`**</span>."""

'''This function will help in converting the lines text to jsonl format'''

def json_data(text,write):
    write.write({'text':text})
    return 'done'

file = open('Resume_HRG_first_10.jsonl','w')
writer = newlinejson.NLJWriter(file)
# print(files)
for filename in sentences:
    for file in filename:
        data = json_data(file,writer)

text_file = open('101_HRG.txt' ,'r')
print(text_file.read())

"""|--------------------------------------------------------|--------------------------------------------|--------------------------------------------|----------------------------------------|------------------|"""

# import newlinejson
# import fitz
# import re
# import os
# import pandas as pd


# # This function is for PDF to text conversion
# def PDFtoTextconverter(fname):
#     doc = fitz.open(fname)
#     text = ""
#     for page in doc:
#         review = re.sub('^rt|http.+?"', ' ', text) #(@[A-Za-z0-9]+)| ^0-9A-Za-z
#         text = review + str(page.getText())
#         tx = " ".join(text.split())
#         tx = tx.replace('➢','•').replace('❖' ,'•').replace('●','•').replace('✓','•').replace('','•').replace('*','•')
#         tx = tx.replace('◦' ,'•').replace('◆','•').replace('⚫','•')
#         tx = tx.replace('\u200b',' ').replace('\u25aa',' ').replace('\uf0b7',' ').replace('\uf0fc',' ').replace('\uf0b3',' ')
#         tx = tx.replace('\uf0c4',' ').replace('\uf02a',' ').replace('\uf028',' ').replace('\uf0d8',' ')
#     return tx


# def json_data(filename,text,write):
#     write.write({filename.split('.')[0]:text})
#     return 'done'


# for root, dirs,files in os.walk('D:/Kreaas Project/Resume Screening Project_01/Resume Parsing model/Data/HRG_Resume'):
#     file = open('test.jsonl','w')
#     writer = newlinejson.NLJWriter(file)
#     for filename in range(101,201):
#         data = {}
#         if filename in range(101,201):
#             filename = '{0}_HRG.pdf'.format(filename)
#             path = os.path.join(root,filename)
#             text = PDFtoTextconverter(path).lower()
#             data = json_data(filename.split('.')[0],text,writer)



nlp = spacy.load('en_core_web_lg') # or whatever model you have installed
raw_text = '''g.raghu shankar hyderabad, telangana raghuhr203_ifi@indeedemail.com 9032564088
• over 11+ years of work experience in human resource domain focused in it recruitment & non it
recruitment for different leading organizations. • solid experience in recruitment, leadership hiring,
talent acquisition, resource management, , team management. • experience in internal recruitments,
contractual staffing, management / supervision of recruiters, employee relations, salary negotiations,
strategic recruitment initiatives etc. • excellent multi-tasking and proactive solution skills with a
wide variety of recruiting, management, and technical backgrounds to successfully benefit and generously
add results to any work environment. • experience in writing job descriptions, job specifications and job
adverts for clients. • handling a team of recruiters. competencies anchoring my successes include:
• executive/technical/professional recruiter: 11+ years of successful placements, matching the "needs" of
the employer with the "best" qualified candidates/applicants by: • conducting job analysis, job evaluation
resulting in quality job specifications and workflow benchmarks; • candidate screening; assisted clients
with conducting background checks; verification of references, extensive screening; attribute assessment
screening utilizing a variety of tools including profiles. work experience team leader - recruitment star
powerz human resources pvt. ltd october 2015 to present responsibilities • handling the recruitment team.
• mentoring team members. • client handling. • collecting the requirements from the clients and allocating
them to the team. • making them understand the requirements in detail and helping them in sourcing.
• checking the quality of the profiles which will be sent by the team • taking care of the hr issues
while screening the resume in terms of the present employer, years of experience, relevant work location,
relevant background, relevant education, etc • processing the profiles to the client and taking care of the
feedback. • scheduling the f-f interview in client location or else telephonic/zoom interview with the client
depends upon their feasibility. • structuring and negotiation of compensation with successful candidates,
agency vendors and hiring managers • coordinating during the entire process till the candidate joins the company.
• taking care of internal hiring. • conducting walk-in drives for multiple positions.
• providing regular reporting updates and attending appropriate business management meetings
• handling queries with regards to recruitment, hiring & joining. • generating weekly mis reports
to analyse the offers released by respective division. • maintaining recruitment database and candidate
records • once the joined candidate completes 90 days raise the invoices for the same, courier the hard copies and share the scan copies of invoices to client and follow up the client until payments are released. • new client acquisition • making the agreements with the clients. if the agreement expires follow up the client for agreement renewal by providing necessary documents required for renewal of agreement. worked with client like: • kotak life insurance, bharti axa life insurance co.ltd, hdfc life insurance, aditya birla sunlife insurance, tata-aia life insurance, aviva life insurance, aegon religare life insurance, ing vysya life insurance, kotak mahindra bank, hdfc bank, vodafone, transworld international, pm relocations, startrek logistics, sc johnson, nivea, nippon paints, tibarumal jewellers, krishna pearls and jewels, mussadilal jewels, suprabath roller flour mills. industry exposure: bfsi, bpo/ites, freight & logistics, pharma, food processing, fmcg, consumer durables, retail, financial services, marketing, sales. hiring level: associate/senior associate sales, teamleader sales, assistant manager- sales, manager- sales, branch managers, head sales, executive operations, executive hrs, digital marketing managers, relationship managers, client servicing managers, accountants, receptionists, front office exevutives, store managers. star powerz human resources pvt. ltd - hyderabad, telangana october 2009 to present starpowerz human resources pvt. ltd. a professionally managed entity incorporated in 1997 is a hyderabad based services company with proven competencies in recruitment, outsourcing and strategy consulting. hr- executive star powerz human resources pvt. ltd october 2009 to september 2015 responsibilities • working as an individual contributor responsible for sourcing potential candidates through different sources (job portal, social media, advertisements, networking, referrals) • review applications and conduct the initial round of interview to check fitment and provide the necessary information about the job. • handling end-to-end recruitment process. • liaising with hiring managers for understanding the role to identify the best talent. • working closely with the senior management team (state head, cluster head and branch managers) to understand the business requirements, translate their needs and provide them with the very best talent. • successful track record in closing the niche positions. • social media hiring: for hiring senior and niche candidates. • client management: single point of contact for all clients. working with clients to fulfil immediate requirements. regular follow-up with clients on the new requirements, which are to be fulfilled through them. managing and maintaining database for smooth process. • mapping & pipeline: foreseeing the hiring needs and building pipeline through mapping, candidates. • recruitment drives: successfully conducting recruitment drives to close positions with in tat. education b.com osmania university 2004 skills / it skills • full cycle of recruiting & sourcing • expert in sourcing • application tracking system • team management • account management • productivity • vendor management • working experience on windows xp, windows 2007, windows 2010 • proficient in ms-office (word, excel, power point) • expert with internet concepts • working experience on outlook express, outlook 2007, outlook 2010, outlook 2016 • recruitment, hr • end to end recruitment languages • english, hindi, telugu - expert awards / achievements won 1 gram gold coin october 2012 won 1 gram gold coin from the director for my performance in recruitment in the year 2012. awarded with 100 gms silver coin and one gram gold coin for completion of services of a decade in star powerz human resources pvt. ltd'''
doc = nlp(raw_text)
sentences = [sent.text.strip() for sent in doc.sents]



print(len(sentences))

print("===================")
sentences[0][1]

# from ..language import Language
from spacy.language import Language
nlp=spacy.load('en_core_web_lg')

@Language.factory(
    "sentencizer",
    assigns=["token.is_sent_start", "doc.sents"],
    default_config={"punct_chars": None},
    default_score_weights={"sents_f": 1.0, "sents_p": 0.0, "sents_r": 0.0},
)

def set_custom_boundaries(doc):
#     @Language.component
    for token in doc[:-1]:
        if token.text == ".(" or token.text == ").":
            doc[token.i+1].is_sent_start = True
        elif token.text == "Rs." or token.text == ")":
            doc[token.i+1].is_sent_start = False
    return doc

nlp.add_pipe(set_custom_boundaries, before="parser")
doc = nlp('''g.raghu shankar hyderabad, telangana raghuhr203_ifi@indeedemail.com 9032564088
• over 11+ years of work experience in human resource domain focused in it recruitment & non it
recruitment for different leading organizations. • solid experience in recruitment, leadership hiring,
talent acquisition, resource management, , team management.
• experience in internal recruitments, contractual staffing, management / supervision of recruiters, employee relations,
salary negotiations, strategic recruitment initiatives etc.
• excellent multi-tasking and proactive solution skills with a wide variety of recruiting, management, and
technical backgrounds to successfully benefit and generously add results to any work environment.
• experience in writing job descriptions, job specifications and job adverts for clients.
• handling a team of recruiters. competencies anchoring my successes include:
• executive/technical/professional recruiter: 11+ years of successful placements,
matching the "needs" of the employer with the "best" qualified candidates/applicants by:
• conducting job analysis, job evaluation resulting in quality job specifications and workflow benchmarks;
• candidate screening; assisted clients with conducting background checks; verification
of references, extensive screening; attribute assessment screening utilizing a variety of tools including profiles.
work experience team leader - recruitment star powerz human resources pvt. ltd october 2015 to present responsibilities
• handling the recruitment team. • mentoring team members. • client handling.
• collecting the requirements from the clients and allocating them to the team.
• making them understand the requirements in detail and helping them in sourcing. • checking the quality of the profiles which will be sent by the team • taking care of the hr issues while screening the resume in terms of the present employer, years of experience, relevant work location, relevant background, relevant education, etc • processing the profiles to the client and taking care of the feedback. • scheduling the f-f interview in client location or else telephonic/zoom interview with the client depends upon their feasibility. • structuring and negotiation of compensation with successful candidates, agency vendors and hiring managers • coordinating during the entire process till the candidate joins the company. • taking care of internal hiring. • conducting walk-in drives for multiple positions. • providing regular reporting updates and attending appropriate business management meetings • handling queries with regards to recruitment, hiring & joining. • generating weekly mis reports to analyse the offers released by respective division. • maintaining recruitment database and candidate records • once the joined candidate completes 90 days raise the invoices for the same, courier the hard copies and share the scan copies of invoices to client and follow up the client until payments are released. • new client acquisition • making the agreements with the clients. if the agreement expires follow up the client for agreement renewal by providing necessary documents required for renewal of agreement. worked with client like: • kotak life insurance, bharti axa life insurance co.ltd, hdfc life insurance, aditya birla sunlife insurance, tata-aia life insurance, aviva life insurance, aegon religare life insurance, ing vysya life insurance, kotak mahindra bank, hdfc bank, vodafone, transworld international, pm relocations, startrek logistics, sc johnson, nivea, nippon paints, tibarumal jewellers, krishna pearls and jewels, mussadilal jewels, suprabath roller flour mills. industry exposure: bfsi, bpo/ites, freight & logistics, pharma, food processing, fmcg, consumer durables, retail, financial services, marketing, sales. hiring level: associate/senior associate sales, teamleader sales, assistant manager- sales, manager- sales, branch managers, head sales, executive operations, executive hrs, digital marketing managers, relationship managers, client servicing managers, accountants, receptionists, front office exevutives, store managers. star powerz human resources pvt. ltd - hyderabad, telangana october 2009 to present starpowerz human resources pvt. ltd. a professionally managed entity incorporated in 1997 is a hyderabad based services company with proven competencies in recruitment, outsourcing and strategy consulting. hr- executive star powerz human resources pvt. ltd october 2009 to september 2015 responsibilities • working as an individual contributor responsible for sourcing potential candidates through different sources (job portal, social media, advertisements, networking, referrals) • review applications and conduct the initial round of interview to check fitment and provide the necessary information about the job. • handling end-to-end recruitment process. • liaising with hiring managers for understanding the role to identify the best talent. • working closely with the senior management team (state head, cluster head and branch managers) to understand the business requirements, translate their needs and provide them with the very best talent. • successful track record in closing the niche positions. • social media hiring: for hiring senior and niche candidates. • client management: single point of contact for all clients. working with clients to fulfil immediate requirements. regular follow-up with clients on the new requirements, which are to be fulfilled through them. managing and maintaining database for smooth process. • mapping & pipeline: foreseeing the hiring needs and building pipeline through mapping, candidates. • recruitment drives: successfully conducting recruitment drives to close positions with in tat. education b.com osmania university 2004 skills / it skills • full cycle of recruiting & sourcing • expert in sourcing • application tracking system • team management • account management • productivity • vendor management • working experience on windows xp, windows 2007, windows 2010 • proficient in ms-office (word, excel, power point) • expert with internet concepts • working experience on outlook express, outlook 2007, outlook 2010, outlook 2016 • recruitment, hr • end to end recruitment languages • english, hindi, telugu - expert awards / achievements won 1 gram gold coin october 2012 won 1 gram gold coin from the director for my performance in recruitment in the year 2012. awarded with 100 gms silver coin and one gram gold coin for completion of services of a decade in star powerz human resources pvt. ltd''')

for sent in doc.sents:
 print(sent.text)

matcher = Matcher(nlp.vocab , validate = True)

def get_name(resume_text):
    '''This helper function will help to Extract names
    from the text'''

    nlp_text = nlp(resume_text)
    # First name and Last name are always Proper Nouns
    pattern = [{"POS": "PROPN"}, {"POS": "PROPN"}] # Creating a Rule
    matcher.add('NAME' ,[pattern]) # Defining the Pattern
    matches = matcher(nlp_text) # find words and phrases using rules and defined pattern

    for _,start,end in matches:
        span = nlp_text[start:end][:10]
        if 'name' not in span.text.lower():
            return span.text

# get_name()

def get_phone(resume_text , custom_regex = None):
    '''This function will help to get phone number, Regex is being found
    on this link - https://zapier.com/blog/extract-links-email-phone-regex/ '''
    if not custom_regex:
        mob_num_regex = r'''(\d{3}[-\.\s]??\d{3}[-\.\s]??\d{4}|\(\d{3}\)
                        [-\.\s]*\d{3}[-\.\s]??\d{4}|\d{3}[-\.\s]??\d{4})'''
        phone = re.findall(re.compile(mob_num_regex) , resume_text)
    else:
        phone = re.findall(re.compile(custom_regex), resume_text)
    if phone:
        number = ''.join(phone[0])
        return number

# get_phone()

def get_email(resume_text):
    '''
    Helper function to extract email id from text
    '''
    email = re.findall(r"([^@|\s]+@[^@]+\.[^@|\s]+)", resume_text)
    if email:
        try:
            return email[0].split()[0].strip(';')
        except IndexError:
            return None

# get_email()

# nlp_text = nlp(resume_text)
# noun_chunks = list(nlp_text.noun_chunks)
def get_skills(resume_text):
    nlp_text = nlp(resume_text) #Loading the spacy english model
    noun_chunks = list(nlp_text.noun_chunks) # creating a list of base noun phrases

    #Word tokenization and removing stop words
    tokens = [token.text for token in nlp_text if not token.is_stop]

    data = pd.read_csv('skill_python.csv')
    skills = list(data.columns.values) #Extract values

    skillset=[]

    #One-gram checking
    for token in tokens:
        if token.lower() in skills:
            skillset.append(token)

    #Bi-gram and Tri-Gram check
    for token in noun_chunks:
        token = token.text.lower().strip()
        if token in skills:
            skillset.append(token)

    return [i.capitalize() for i in set([i.lower() for i in skillset])]

nlp_text = nlp('Maintain the employee database and leave records')
noun_chunks = list(nlp_text.noun_chunks)
noun_chunks

STOPWORDS = set(stopwords.words('english'))

# Education Degrees
EDUCATION = [
            'MBA','mba','b.com','bcom','B.com','MTECH','mtech','M.TECH','master of business administration'
            'BE','be','B.E.','ME','me','M.E.','M.S.','MSW','B.Com','bachelors of science'
            'BTECH', 'B.TECH','b.tech','PHD','P.HD','phd','p.hd','MSW','B.A',
            'MBA','M.B.A','B.Sc','BSC','M.C.A.','MCA','marketing','human resource','bachelor of commerce'
            ]

def get_education(resume_text):
    nlp_text = nlp(resume_text)
#     nlp_text = [token.text for token in nlp_text if not token.is_stop]
    nlp_text = [sent.text.strip() for sent in nlp_text.sents]

    edu = {}
    # Extract education degree
    for index, text in enumerate(nlp_text):
        for tex in text.split():
            # Replace all special symbols
            tex = re.sub(r'[?|$|.|!|,]', r'', tex)
            if tex.upper() in EDUCATION and tex not in STOPWORDS:
                edu[tex] = text + nlp_text[index]


        # Extract year
    grad = []
    for key in edu.keys():
        year = re.search(re.compile(r'(((20|19)(\d{2})))'), edu[key])
        if year:
            grad.append((key, ''.join(year[0])))
        else:
            grad.append(key)
        return grad

# creating dict for all the PDF files and developing nested dictionary and develloping features
resume={}
for fname in pdf_files:
    key = fname.split("\\")[-1].split(".")[0]
    resume[key] = {}
    nlp_text = PDFtoTextconverter(fname)
    name =  get_name(nlp_text)
    phone = get_phone(nlp_text)
    email = get_email(nlp_text)
    skills = get_skills(nlp_text)[:4]
    education = get_education(nlp_text)
    resume[key]['name'] = name
    resume[key]['phone'] = phone
    resume[key]['email'] = email
    resume[key]['skills'] = skills
    resume[key]['education'] = education

# resume

df = pd.DataFrame.from_dict(resume).T
df.tail(50)

# for files in pdf_files:
# fname = 'D:/Kreaas Project/Resume Screening Project_01/Resume Parsing model/100 Resumes(PDF)\\99_PD.pdf'
# nlp_text = PDFtoTextconverter(fname)
# nlp_text

# resume[7]

# for token in doc:
#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
#             token.shape_, token.is_alpha, token.is_stop)

# print(doc)

# fname = '01_HRG'
# nlp_text = PDFtoTextconverter(fname)
# nlp_text

# import requests
# csv_url ='https://raw.githubusercontent.com/OmkarPathak/pyresparser/36f1242fbc0637a6633371ca81cf1aea4fa7497e/pyresparser/skills.csv'
# req= requests.get(csv_url)
# url_content = req.content
# csv_file = open('skill_python.csv', 'wb')

# csv_file.write(url_content)
# csv_file.close()

# # This function is for PDF to text conversion
# def PDFtoTextconverter(fname):
#     doc = fitz.open(fname)
#     text = ""
#     for page in doc:
#         review = re.sub('([▪  • / * • - ●  •  ✔ ⮚ ● -, ❖ : \t])|(\w+:\/\/\S+:▪)|^rt|http.+?"',' ', text) #(@[A-Za-z0-9]+)| ^0-9A-Za-z
#         text = review + str(page.getText())
#     tx = " ".join(text.split('\n'))
#     return tx

# PDFtoTextconverter('04_HRG.pdf')